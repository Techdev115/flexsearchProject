## Documentation 0.7.0

### Language Handler

Handling languages was completely replaced by a more generic approach. All language-specific definitions has excluded and was optimized for maximum dead-code elimination when using compiler/bundler. Each language exists of 6 definitions, which are divided into two groups:

1. Charset (mandatory)
    1. `encode`
    2. `split`
    3. `rtl`
2. Language (optional)
    1. `stemmer`
    2. `filter`
    3. `matcher`
    
Therefore to __fully__ describe a custom language on the fly you need to pass:

```js
const index = FlexSearch({
    // mandatory:
    encode: (str) => str,
    split: /\W+/,
    rtl: false,
    // optionally:
    filter: [],
    stemmer: {},
    matcher: {}
});
```

When passing no parameter it uses the default `latin` schema. <!-- The field `matcher` includes just application-custom matchers which is not a part of the language pack. -->

<table>
    <tr></tr>
    <tr>
        <td>Field</td>
        <td>Category</td>
        <td>Description</td>
    </tr>
    <tr>
        <td><b>encode</b></td>
        <td>charset</td>
        <td>The encoder function.</td>
    </tr>
    <tr></tr>
    <tr>
        <td><b>split</b></td>
        <td>charset</td>
        <td>The expression which splits words.</td>
    </tr>
    <tr></tr>
    <tr>
        <td><b>rtl</b></td>
        <td>charset</td>
        <td>A boolean property which indicates right-to-left encoding.</td>
    </tr>
    <tr></tr>
    <tr>
        <td><b>filter</b></td>
        <td>language</td>
        <td>Filter are also known as "stopwords", they completely filter out words from being indexed.</td>
    </tr>
    <tr></tr>
    <tr>
        <td><b>stemmer</b></td>
        <td>language</td>
        <td>Stemmer removes word endings and is a kind of "partial normalization". A word ending just matched when the word length is bigger than the matched partial.</td>
    </tr>
    <tr></tr>
    <tr>
        <td><b>matcher</b></td>
        <td>language</td>
        <td>Matcher replaces all occurrences of a given string regardless of its position and is also a kind of "partial normalization".</td>
    </tr>
</table>

#### About Tokenizer

In FlexSearch you can't provide your own tokenizer, because it is a direct dependency to the core unit. What in FlexSearch is called "split" is often named the "tokenizer" on other libraries. Of course splitting sentences into words is also a kind of tokenize, but the tokenizer of FlexSearch splits the word itself into chunks (strict, forward, reverse/both, full, ngram).

### 1. Language Packs: ES6 Modules

The goal is to collect language definitions and providing them as built-ins for simple reusing. The folder `/lang/` includes all definitions, also divided by charset and language. Let's take an example on how to make a full qualified initialization of a built-in language:

```js
import { encode, split, rtl } from "./dist/module/lang/latin/index.js";
import { stemmer, filter, matcher } from "./dist/module/lang/en.js";

const index = FlexSearch({
    // charset:
    encode: encode,
    split: split,
    rtl: rtl,
    // language:
    filter: filter,
    stemmer: stemmer,
    matcher: matcher
});
```

The example above is the standard interface which is at least exported from each charset/language.

When passing nothing to the initializer:

```js
const index = FlexSearch();
```

it uses the default `latin` schema by default, pretty much the same like:

```js
import { encode, split } from "./dist/module/lang/latin/index.js";

const index = FlexSearch({
    encode: encode,
    split: split
});
```

#### Encoder Variants

You remember the old encoding variants like `simple`, `advanced`, `extra`, or `balanced`? These are also supported and provides you several variants of encoding (which has different performance and results).

It is pretty straight forward when using a encoder variant:

```js
import { advanced, extra, split } from "./dist/module/lang/latin/index.js";

const index_advanced = FlexSearch({
    encode: advanced,
    split: split
});

const index_extra = FlexSearch({
    encode: extra,
    split: split
});
```

#### Dialect / Slang

Language definitions (especially matchers) also could be used to normalize dialect and slang of a specific language.

### 2. Language Packs: ES5 Modules

When loading language packs, make sure that the library was loaded before:

```html
<script src="dist/flexsearch.min.js"></script>
<script src="dist/lang/latin/default.min.js"></script>
<script src="dist/lang/en.min.js"></script>
```

Because you loading packs as external packages (non-ES6-modules) you have to initialize them this way:

```js
var index = FlexSearch({
    // charset:
    encode: "latin",
    split: "latin",
    rtl: "latin",
    // language:
    filter: "en",
    stemmer: "en",
    matcher: "en"
});
```

Or a shortcut for all charset and language specific settings, e.g. same from above as:

```js
var index = FlexSearch({
    charset: "latin",
    lang: "en"
});
```

#### Encoder Variants

It is pretty straight forward when using an encoder variant:

```html
<script src="dist/flexsearch.min.js"></script>
<script src="dist/lang/latin/advanced.min.js"></script>
<script src="dist/lang/latin/extra.min.js"></script>
<script src="dist/lang/en.min.js"></script>
```

```js
const index_advanced = FlexSearch({
    encode: "latin:advanced",
    split: "latin"
});

const index_extra = FlexSearch({
    encode: "latin:extra",
    split: "latin"
});
```

But here we use the `charset:variant` notation to define charset and its variants. Just using "latin" as the encoder implicitly will resolved as `latin:default` under the hood.

### Language Processing Pipeline

This is the default pipeline provided by FlexSearch:

<p>
    <img src="https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.0/doc/pipeline.svg">
</p>

You can specify your own pipeline or extend it in the upcoming release.

### How to contribute?

Search for your language in `src/lang/`, if it exists you can extend or provide variants (like dialect/slang). If the language doesn't exist create a new file and check if any of the existing charsets (e.g. latin) fits to your language. When no charset exist, you need to provide a charset as a base for the language.

A new charset should provide at least:

1. `encode` A function which normalize the charset (remove special chars, etc.)
2. `split` A regular expression or a function which splits a sentence into words (or similar when language has no word breaks)
3. `rtl` A boolean flag which indicates right-to-left encoding

Basically the charset file looks something like:

```js
export function encode(str){ return str }
export const split = /[\W_]+/;
export const rtl = false;
```